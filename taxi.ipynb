{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c25e0e7",
   "metadata": {},
   "source": [
    "### Value Iteration for Taxi-V3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860b0fd1",
   "metadata": {},
   "source": [
    "Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4947ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87027147",
   "metadata": {},
   "source": [
    "Make the gym environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6345f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f1f84",
   "metadata": {},
   "source": [
    "Define the value iteration function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b8c4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, discount_factor, delta_threshold):\n",
    "\n",
    "    def find_max(state_rewards, V): # find the max value action from a given state\n",
    "\n",
    "        value = -math.inf # initialize to negative infinity\n",
    "        index = 0\n",
    "\n",
    "        for i in range(len(state_rewards)):\n",
    "\n",
    "            d = discount_factor \n",
    "            # if V[state_rewards[i][0]] < 0: # If the value of the state is negative, it becomes MORE negatve after discounting. Results are similar with or without this.\n",
    "            #     d = 2 - d\n",
    "\n",
    "            if value < (state_rewards[i][1] + d * V[state_rewards[i][0]]):\n",
    "                value = state_rewards[i][1] + d * V[state_rewards[i][0]]\n",
    "                index = i\n",
    "\n",
    "        return value, index\n",
    "\n",
    "    num_states = env.observation_space.n\n",
    "\n",
    "    # Initialize the value function\n",
    "    V = np.zeros(num_states)\n",
    "\n",
    "    # For each state, the policy will tell you the action to take\n",
    "    policy = np.zeros(num_states, dtype=int)\n",
    "\n",
    "    env_unwrapped = env.unwrapped # saw this in some random discussion post online, it's necessary to use the P attribute at line 48\n",
    "\n",
    "    while True:\n",
    "\n",
    "        delta = 0 # Initialize delta to 0 for each iteration\n",
    "\n",
    "        for state in range(num_states): # for each state \n",
    "\n",
    "            actions = env_unwrapped.P[state] # get all actions\n",
    "            state_rewards = [(actions[i][0][1], actions[i][0][2]) for i in range(len(actions))] # get the states and rewards for each action from a certain state and store it as a (state, reward) tuple\n",
    "            \n",
    "            v = V[state] # store the current value of the state\n",
    "            V[state], policy[state] = find_max(state_rewards, V) # update the value and policy\n",
    "            delta = max(delta, abs(v - V[state])) # update delta to the max difference between the old and new value of the state\n",
    "\n",
    "        if delta < delta_threshold: # if there were no changes bigger than the threshold, we can stop the iteration\n",
    "            break\n",
    "\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f361e2e",
   "metadata": {},
   "source": [
    "Specify the parameters to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8dfffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor = 0.9\n",
    "delta_threshold = 0.00000001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2e730d",
   "metadata": {},
   "source": [
    "Run the value iteration to obtain the values and best policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a17871ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, V = value_iteration(env, discount_factor, delta_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae4f7ce",
   "metadata": {},
   "source": [
    "Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf9fee45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of steps taken: 14\n"
     ]
    }
   ],
   "source": [
    "# resetting the environment and executing the policy\n",
    "state = env.reset()\n",
    "#state = state[0]\n",
    "step = 0\n",
    "done = False\n",
    "state = state[0]\n",
    "max_steps = 100\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    # Getting max value against that state, so that we choose that action\n",
    "\n",
    "    action = policy[state]\n",
    "    # action = env.action_space.sample() # random action for testing\n",
    "    new_state, reward, done, truncated, info = env.step(action) # information after taking the action\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"number of steps taken:\", step)\n",
    "        break\n",
    "\n",
    "    state = new_state\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
